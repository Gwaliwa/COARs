# pages/chatroom.py
# ðŸ”’ Chatroom â€” Private Q&A on your consolidated file (SESSION-ONLY)
# - Uses ONLY st.session_state["consolidated_df"] (must be a pandas DataFrame with rows)
# - No uploads, no external API calls, no data sharing
# - Refuses questions that aren't grounded in the consolidated content

import re
import numpy as np
import pandas as pd
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# ---------------- Page config ----------------
st.set_page_config(page_title="ðŸ”’ Chatroom (Private Q&A)", layout="wide")
st.title("ðŸ”’ Chatroom")
st.caption("Answers are strictly based on the consolidated file already in memory. No external data or services are used.")

# ---------------- Strict session-only guard ----------------
SESSION_KEY = "consolidated_df"   # <- the ONLY key we accept

# 1) Fetch strictly from one key
df = st.session_state.get(SESSION_KEY, None)

# 2) Debug snapshot (types only, no data)
with st.expander("ðŸ”§ Session debug (keys â†’ types)"):
    try:
        st.json({k: str(type(v)) for k, v in st.session_state.items()})
    except Exception:
        st.write("Unable to show session snapshot.")

# 3) Guard rails
if not isinstance(df, pd.DataFrame):
    st.error(
        f"No DataFrame found at st.session_state['{SESSION_KEY}'].\n\n"
        "Go to your main page (e.g., main1.py) and set:\n"
        "    st.session_state['consolidated_df'] = consolidated_df\n"
        "(Make sure consolidated_df is a REAL pandas DataFrame, not None.)"
    )
    st.stop()

if df.empty:
    st.error(
        f"`{SESSION_KEY}` exists but has 0 rows. Ensure your consolidation produced rows "
        "and reassign it to session."
    )
    st.stop()

st.success(f"Using DataFrame from session: '{SESSION_KEY}' ({df.shape[0]} rows).")

# ---------------- Utilities ----------------
def _coalesce_cols(row: pd.Series, cols):
    parts = []
    for c in cols:
        if c in row and pd.notna(row[c]):
            v = str(row[c]).strip()
            if v:
                parts.append(v)
    return " ".join(parts).strip()

def _sent_split(x: str):
    x = re.sub(r"\s+", " ", str(x).strip())
    if not x:
        return []
    return re.split(r"(?<=[\.\?\!])\s+", x)

def _chunk_text(text: str, max_words: int = 120):
    sents = _sent_split(text)
    chunks, cur, count = [], [], 0
    for s in sents:
        w = len(s.split())
        if count + w > max_words and cur:
            chunks.append(" ".join(cur))
            cur, count = [s], w
        else:
            cur.append(s)
            count += w
    if cur:
        chunks.append(" ".join(cur))
    return [c for c in chunks if c.strip()]

# ---------------- Build chat corpus ----------------
st.markdown("#### Build chat corpus (select the columns to include)")
available_cols = list(df.columns)
default_cols = [c for c in ["context", "contributions", "collaborations", "innovations"] if c in available_cols]
cols_for_chat = st.multiselect(
    "Columns to include for Q&A grounding",
    options=available_cols,
    default=default_cols if default_cols else (available_cols[:4] if len(available_cols) >= 1 else [])
)

if not cols_for_chat:
    st.warning("Select at least one column to include in the chat corpus.")
    st.stop()

docs_df = pd.DataFrame({
    "filename": df["filename"] if "filename" in df.columns else np.arange(len(df)).astype(str),
    "text": df.apply(lambda r: _coalesce_cols(r, cols_for_chat), axis=1)
})

# chunk documents into ~120-word passages
passages = []
for i, row in docs_df.iterrows():
    chs = _chunk_text(row["text"], max_words=120)
    for j, ch in enumerate(chs):
        passages.append({
            "pid": f"{i}-{j}",
            "filename": row["filename"],
            "chunk_index": j,
            "text": ch
        })

passages_df = pd.DataFrame(passages)
if passages_df.empty:
    st.error("No text found in the selected columns to build the chat corpus.")
    st.stop()

# ---------------- Local vector index (no internet, no external sharing) ----------------
@st.cache_resource(show_spinner=False)
def build_vector_index(pass_df: pd.DataFrame):
    stop_words = {
        "the","and","to","of","in","for","on","with","a","an","is","are","was","were","be","by","as","that","this","it",
        "from","or","at","we","our","their","they","these","those","has","have","had","not","but","can","will","may"
    }
    vectorizer = TfidfVectorizer(stop_words=sorted(stop_words), max_df=0.95, ngram_range=(1,2))
    X = vectorizer.fit_transform(pass_df["text"].values)
    return vectorizer, X

vectorizer, X_passages = build_vector_index(passages_df)

# ---------------- Privacy & scope rules ----------------
with st.expander("ðŸ”’ Privacy & scope (read-only, local)"):
    st.markdown("""
- Answers are generated only from the consolidated file loaded in memory.
- No external services, models, or APIs are called.
- If a question is unrelated to the consolidated content, I will decline.
- I will not export or transmit the data elsewhere.
""")

# ---------------- Chat state ----------------
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

def _add_msg(role, content):
    st.session_state.chat_history.append({"role": role, "content": content})

def _clear_chat():
    st.session_state.chat_history = []

# ---------------- Retrieval + answering ----------------
def _retrieve(query: str, top_k: int = 5):
    q_vec = vectorizer.transform([query])
    sims = cosine_similarity(q_vec, X_passages).ravel()
    idx = np.argsort(-sims)[:top_k]
    return passages_df.iloc[idx].assign(score=sims[idx])

def _in_scope(results_df: pd.DataFrame, min_max_sim: float = 0.18):
    return (not results_df.empty) and (results_df["score"].max() >= min_max_sim)

def _synthesize(query: str, results_df: pd.DataFrame, max_snippets: int = 3, max_chars: int = 900):
    out_sents = []
    for _, row in results_df.head(max_snippets).iterrows():
        chunk = row["text"]
        sents = _sent_split(chunk)
        key_terms = [w for w in re.findall(r"[A-Za-z]{3,}", query.lower())]
        chosen = []
        for s in sents:
            low = s.lower()
            hits = sum(1 for t in key_terms if t in low)
            if hits:
                chosen.append((hits, s))
        if not chosen and sents:
            chosen = [(0, sents[0])]
        chosen = [s for _, s in sorted(chosen, key=lambda x: -x[0])[:2]]
        out_sents.extend(chosen)

    answer = " ".join(out_sents).strip()
    if len(answer) > max_chars:
        answer = answer[:max_chars].rsplit(" ", 1)[0] + "â€¦"

    sources = list(dict.fromkeys(results_df["filename"].tolist()))
    return answer, sources

# ---------------- Sidebar ----------------
with st.sidebar:
    st.header("Chat settings")
    top_k = st.slider("Top-k passages", 3, 10, 5, 1)
    sim_gate = st.slider("Similarity gate (refuse if below)", 0.05, 0.50, 0.18, 0.01)
    st.button("ðŸ§¹ Clear chat", on_click=_clear_chat)

# ---------------- Render chat history ----------------
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# ---------------- Input ----------------
user_q = st.chat_input("Ask a question about the consolidated fileâ€¦")
if user_q:
    _add_msg("user", user_q)
    with st.chat_message("user"):
        st.write(user_q)

    lower_q = user_q.lower()
    blocked = [
        "send", "email", "upload", "share", "post", "publish", "external", "internet", "api", "slack", "teams",
        "full dataset", "entire dataset", "raw data", "all rows", "download everything"
    ]
    if any(p in lower_q for p in blocked):
        refusal = "I canâ€™t share or export data externally. I can only answer questions grounded in the consolidated file."
        _add_msg("assistant", refusal)
        with st.chat_message("assistant"):
            st.write(refusal)
    else:
        hits = _retrieve(user_q, top_k=top_k)
        if not _in_scope(hits, min_max_sim=sim_gate):
            out = "That seems outside the scope of the consolidated file. Please ask about information contained in it."
            _add_msg("assistant", out)
            with st.chat_message("assistant"):
                st.write(out)
        else:
            answer, sources = _synthesize(user_q, hits, max_snippets=min(3, top_k))
            if not answer:
                answer = "I found relevant passages but couldnâ€™t synthesize a concise answer. Try asking more specifically."
            source_str = ", ".join(sources[:5])
            final = f"{answer}\n\n**Sources (filenames):** {source_str}"
            _add_msg("assistant", final)
            with st.chat_message("assistant"):
                st.markdown(final)

